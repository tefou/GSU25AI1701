import sys
import os
# ==== S·ª≠a ƒë∆∞·ªùng d·∫´n cho import module custom ====
sys.path.append(os.path.abspath("."))           # Th∆∞ m·ª•c g·ªëc d·ª± √°n
sys.path.append(os.path.abspath("./R3GAN"))     # ƒê·ªÉ import ƒë∆∞·ª£c torch_utils + R3GAN.R3GAN

import streamlit as st
import pandas as pd
import numpy as np
import cv2
from PIL import Image
import torch
import pickle
import json
import xml.etree.ElementTree as ET
from streamlit_drawable_canvas import st_canvas

# ===== 1. Import Generator chu·∫©n t·ª´ R3GAN =====
from R3GAN.Networks import Generator

# ===== 2. Build Generator v·ªõi ƒë√∫ng tham s·ªë =====
def build_generator():
    return Generator(
        InputChannels=3,
        WidthPerStage=[32, 64, 64, 32],
        CardinalityPerStage=[2, 4, 4, 2],
        BlocksPerStage=[1, 1, 1, 1],
        ExpansionFactor=2,
        KernelSize=3,
        ResamplingFilter=[1, 2, 1]
    )

# ===== 3. Load state_dict t·ª´ .pkl =====
def load_generator_state_dict(pkl_path):
    with open(pkl_path, "rb") as f:
        data = pickle.load(f, encoding="latin1")
    G_ema = data.get('G_ema', None)
    state_dict = None
    if G_ema is not None:
        if hasattr(G_ema, 'state_dict'):
            state_dict = G_ema.state_dict()
        elif isinstance(G_ema, dict) and 'state_dict' in G_ema:
            state_dict = G_ema['state_dict']
        elif hasattr(G_ema, '__dict__'):
            state_dict = G_ema.__dict__.get('state_dict', None)
    return state_dict

@st.cache_resource
def load_model():
    model_path = "network-snapshot-000000008.pkl"
    try:
        sd = load_generator_state_dict(model_path)
        if sd is None:
            st.error("‚ùå Kh√¥ng l·∫•y ƒë∆∞·ª£c state_dict t·ª´ model.")
            return None
        gen = build_generator()
        gen.load_state_dict(sd, strict=False)
        gen.eval()
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        gen = gen.to(device)
        st.success("‚úÖ Model loaded th√†nh c√¥ng!")
        return gen
    except Exception as e:
        st.error(f"‚ùå L·ªói load model: {e}")
        return None

def preprocess_image_region(image_region):
    # N·∫øu ·∫£nh l√† 2D (grayscale), convert sang RGB (3 k√™nh)
    if len(image_region.shape) == 2:
        image_region = cv2.cvtColor(image_region, cv2.COLOR_GRAY2RGB)
    # N·∫øu l√† BGR (do OpenCV), ƒë·ªïi v·ªÅ RGB
    if image_region.shape[2] == 3:
        pass  # gi·ªØ nguy√™n
    resized = cv2.resize(image_region, (256,256), interpolation=cv2.INTER_CUBIC)
    # Normalize v·ªÅ [-1, 1]
    normalized = (resized.astype(np.float32) / 127.5) - 1.0
    tensor = torch.FloatTensor(normalized).permute(2, 0, 1).unsqueeze(0)  # [1, 3, 256, 256]
    return tensor

def postprocess_output(output_tensor):
    output = (output_tensor.squeeze().detach().cpu().numpy() + 1.0) * 127.5
    output = np.clip(output, 0, 255).astype(np.uint8)
    # ƒê·∫£m b·∫£o output shape l√† (H, W, 3) n·∫øu c√≥ 3 k√™nh
    if output.ndim == 3 and output.shape[0] == 3:
        output = np.transpose(output, (1, 2, 0))
    return output

def enhance_image_region(model, image_region):
    try:
        device = next(model.parameters()).device
        input_tensor = preprocess_image_region(image_region).to(device)
        with torch.no_grad():
            out = model(input_tensor)
        enhanced_image = postprocess_output(out)
        return enhanced_image
    except Exception as e:
        st.error(f"L·ªói khi enhance: {str(e)}")
        return None

def crop_center(img, box, crop_size=32):
    x1, y1, x2, y2 = box
    cx = (x1 + x2) // 2
    cy = (y1 + y2) // 2
    h, w = img.shape[:2]
    half = crop_size // 2
    left = min(max(cx - half, 0), w - crop_size)
    top = min(max(cy - half, 0), h - crop_size)
    # ƒê·∫£m b·∫£o kh√¥ng out of bounds
    left = max(0, left)
    top = max(0, top)
    right = min(left + crop_size, w)
    bottom = min(top + crop_size, h)
    crop = img[top:bottom, left:right]
    # N·∫øu thi·∫øu shape, pad l·∫°i
    if crop.shape[0] != crop_size or crop.shape[1] != crop_size:
        crop = cv2.copyMakeBorder(
            crop, 0, crop_size - crop.shape[0], 0, crop_size - crop.shape[1], borderType=cv2.BORDER_REFLECT)
    # N·∫øu c√≤n thi·∫øu k√™nh m√†u, convert sang RGB
    if len(crop.shape) == 2:
        crop = cv2.cvtColor(crop, cv2.COLOR_GRAY2RGB)
    elif crop.shape[2] == 1:
        crop = cv2.cvtColor(crop, cv2.COLOR_GRAY2RGB)
    return crop

def calculate_display_size(orig_w, orig_h, max_size=600):
    """T√≠nh to√°n k√≠ch th∆∞·ªõc hi·ªÉn th·ªã gi·ªØ nguy√™n t·ª∑ l·ªá"""
    if orig_w > orig_h:
        if orig_w > max_size:
            display_w = max_size
            display_h = int(orig_h * max_size / orig_w)
        else:
            display_w = orig_w
            display_h = orig_h
    else:
        if orig_h > max_size:
            display_h = max_size
            display_w = int(orig_w * max_size / orig_h)
        else:
            display_w = orig_w
            display_h = orig_h
    return display_w, display_h

def extract_boxes(canvas_result, display_w, display_h, orig_w, orig_h):
    """Tr√≠ch xu·∫•t t·∫•t c·∫£ bounding boxes (kh√¥ng gi·ªõi h·∫°n s·ªë l∆∞·ª£ng)"""
    boxes = []
    if canvas_result.json_data is not None:
        objects = canvas_result.json_data.get("objects", [])
        for obj in objects:
            if obj["type"] == "rect":
                left = int(obj["left"] * orig_w / display_w)
                top = int(obj["top"] * orig_h / display_h)
                width = int(obj["width"] * orig_w / display_w)
                height = int(obj["height"] * orig_h / display_h)
                x1, y1, x2, y2 = left, top, left+width, top+height
                boxes.append((x1, y1, x2, y2))
    return boxes

# ===== ANNOTATION PARSING FUNCTIONS =====
def parse_csv_annotation(csv_file):
    """
    Parse CSV annotation file - ƒê√∫ng c·∫•u tr√∫c: 
    rad_ID, class_name, x_min, y_min, x_max, y_max, class_id
    """
    try:
        df = pd.read_csv(csv_file)
        df.columns = df.columns.str.strip()
        required_cols = ['rad_ID', 'class_name', 'x_min', 'y_min', 'x_max', 'y_max', 'class_id']
        for col in required_cols:
            if col not in df.columns:
                return None, f"Thi·∫øu c·ªôt b·∫Øt bu·ªôc: {col}"
        boxes = []
        classes = []
        for _, row in df.iterrows():
            x1 = int(float(row['x_min']))
            y1 = int(float(row['y_min']))
            x2 = int(float(row['x_max']))
            y2 = int(float(row['y_max']))
            boxes.append((x1, y1, x2, y2))
            # L∆∞u class_name/class_id ƒë·ªÉ hi·ªÉn th·ªã n·∫øu c·∫ßn
            classes.append({
                "rad_ID": row['rad_ID'],
                "class_name": row['class_name'],
                "class_id": row['class_id']
            })
        return (boxes, classes), f"T√¨m th·∫•y {len(boxes)} annotation(s)"
    except Exception as e:
        return None, f"L·ªói parse CSV: {str(e)}"


def parse_json_annotation(json_file, image_filename):
    """Parse JSON annotation file - supports COCO, custom JSON formats"""
    try:
        data = json.load(json_file)
        
        # COCO format
        if 'annotations' in data and 'images' in data:
            # Find image ID
            image_id = None
            for img in data['images']:
                if image_filename in img['file_name']:
                    image_id = img['id']
                    break
            
            if image_id is None:
                return None, f"Kh√¥ng t√¨m th·∫•y ·∫£nh {image_filename} trong COCO JSON"
            
            # Get annotations for this image
            boxes = []
            for ann in data['annotations']:
                if ann['image_id'] == image_id:
                    bbox = ann['bbox']  # [x, y, width, height]
                    x, y, w, h = bbox
                    boxes.append((int(x), int(y), int(x + w), int(y + h)))
            
            return boxes, f"T√¨m th·∫•y {len(boxes)} annotation(s) (COCO format)"
        
        # Custom JSON format (assume direct structure)
        elif image_filename in data or 'annotations' in data:
            boxes = []
            annotations = data.get(image_filename, data.get('annotations', []))
            
            for ann in annotations:
                if 'bbox' in ann:
                    bbox = ann['bbox']
                    if len(bbox) == 4:
                        x, y, w, h = bbox
                        boxes.append((int(x), int(y), int(x + w), int(y + h)))
                elif all(key in ann for key in ['x', 'y', 'width', 'height']):
                    x, y, w, h = ann['x'], ann['y'], ann['width'], ann['height']
                    boxes.append((int(x), int(y), int(x + w), int(y + h)))
            
            return boxes, f"T√¨m th·∫•y {len(boxes)} annotation(s) (Custom JSON)"
        
        return None, "Format JSON kh√¥ng ƒë∆∞·ª£c h·ªó tr·ª£"
        
    except Exception as e:
        return None, f"L·ªói parse JSON: {str(e)}"

def parse_xml_annotation(xml_file, image_filename):
    """Parse XML annotation file - supports PASCAL VOC format"""
    try:
        tree = ET.parse(xml_file)
        root = tree.getroot()
        
        # Check if this XML is for the right image
        filename_elem = root.find('filename')
        if filename_elem is not None and image_filename not in filename_elem.text:
            return None, f"XML kh√¥ng kh·ªõp v·ªõi ·∫£nh {image_filename}"
        
        boxes = []
        for obj in root.findall('object'):
            bbox = obj.find('bndbox')
            if bbox is not None:
                xmin = int(bbox.find('xmin').text)
                ymin = int(bbox.find('ymin').text)
                xmax = int(bbox.find('xmax').text)
                ymax = int(bbox.find('ymax').text)
                boxes.append((xmin, ymin, xmax, ymax))
        
        return boxes, f"T√¨m th·∫•y {len(boxes)} annotation(s) (PASCAL VOC)"
        
    except Exception as e:
        return None, f"L·ªói parse XML: {str(e)}"

def parse_txt_annotation(txt_file, image_filename, img_width, img_height):
    """Parse TXT annotation file - supports YOLO format"""
    try:
        lines = txt_file.read().decode('utf-8').strip().split('\n')
        
        boxes = []
        for line in lines:
            if line.strip():
                parts = line.strip().split()
                if len(parts) >= 5:
                    # YOLO format: class_id center_x center_y width height (normalized)
                    _, cx, cy, w, h = parts[:5]
                    cx, cy, w, h = float(cx), float(cy), float(w), float(h)
                    
                    # Convert from normalized to absolute coordinates
                    x1 = int((cx - w/2) * img_width)
                    y1 = int((cy - h/2) * img_height)
                    x2 = int((cx + w/2) * img_width)
                    y2 = int((cy + h/2) * img_height)
                    
                    boxes.append((x1, y1, x2, y2))
        
        return boxes, f"T√¨m th·∫•y {len(boxes)} annotation(s) (YOLO format)"
        
    except Exception as e:
        return None, f"L·ªói parse TXT: {str(e)}"

def visualize_annotations(image_np, boxes, classes=None):
    """Visualize bounding boxes + class_name l√™n ·∫£nh"""
    img_vis = image_np.copy()
    for i, (x1, y1, x2, y2) in enumerate(boxes):
        cv2.rectangle(img_vis, (x1, y1), (x2, y2), (255, 0, 0), 2)
        label = f'Region {i+1}'
        if classes is not None and len(classes) > i:
            label += f": {classes[i]['class_name']}"
        cv2.putText(img_vis, label, (x1, max(10, y1-10)),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 1)
    return img_vis


# ========== MAIN STREAMLIT APP ==========
st.markdown('<h1 style="text-align:center">ü©ª Demo TƒÉng C∆∞·ªùng Ch·∫•t L∆∞·ª£ng ·∫¢nh X-RAY b·∫±ng R3GAN</h1>', unsafe_allow_html=True)

# ===== MODEL STATUS =====
st.markdown("---")
st.header("ü§ñ Tr·∫°ng th√°i Model")
model = load_model()

if not model:
    st.error("‚ùå Model ch∆∞a ƒë∆∞·ª£c load th√†nh c√¥ng")
    st.stop()

# ===== TABS =====
tab1, tab2 = st.tabs(["üé® Khoanh V√πng Th·ªß C√¥ng", "üìä Khoanh V√πng T·ª± ƒê·ªông"])

# ========== TAB 1: MANUAL ENHANCEMENT ==========
with tab1:
    st.header("üìã H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng")
    with st.expander("üëâ Xem h∆∞·ªõng d·∫´n chi ti·∫øt", expanded=False):
        st.markdown("""
        **B∆∞·ªõc 1:** Upload ·∫£nh X-ray c·ªßa b·∫°n
        
        **B∆∞·ªõc 2:** V·∫Ω c√°c v√πng c·∫ßn tƒÉng c∆∞·ªùng
        
        **B∆∞·ªõc 3:** Nh·∫•n n√∫t "üöÄ TƒÉng C∆∞·ªùng" ƒë·ªÉ x·ª≠ l√Ω t·∫•t c·∫£ v√πng
        """)

    # ===== UPLOAD ·∫¢NH =====
    st.markdown("---")
    st.header("üì§ Upload ·∫£nh X-ray")
    uploaded_file = st.file_uploader("Ch·ªçn ·∫£nh X-ray", type=["png", "jpg", "jpeg", "bmp", "tiff"], key="manual_upload")

    if not uploaded_file:
        st.info("üìÅ Vui l√≤ng upload ·∫£nh X-ray ƒë·ªÉ b·∫Øt ƒë·∫ßu")
    else:
        # Load v√† hi·ªÉn th·ªã ·∫£nh g·ªëc
        image = Image.open(uploaded_file).convert("RGB")
        image_np = np.array(image)

        # T√≠nh to√°n k√≠ch th∆∞·ªõc hi·ªÉn th·ªã gi·ªØ nguy√™n t·ª∑ l·ªá
        orig_w, orig_h = image.size
        display_w, display_h = calculate_display_size(orig_w, orig_h)

        st.success(f"‚úÖ ƒê√£ upload th√†nh c√¥ng! K√≠ch th∆∞·ªõc g·ªëc: {orig_w}x{orig_h}")
        st.info(f"üìê K√≠ch th∆∞·ªõc hi·ªÉn th·ªã: {display_w}x{display_h}")

        # ===== V·∫º V√ôNG C·∫¶N X·ª¨ L√ç =====
        st.markdown("---")
        st.header("üñ±Ô∏è V·∫Ω v√πng c·∫ßn x·ª≠ l√Ω")
        st.markdown("**H∆∞·ªõng d·∫´n:** K√©o chu·ªôt ƒë·ªÉ t·∫°o h√¨nh ch·ªØ nh·∫≠t quanh c√°c v√πng b·∫°n mu·ªën tƒÉng ch·∫•t l∆∞·ª£ng. ")

        # Canvas ƒë·ªÉ v·∫Ω bounding boxes
        canvas_result = st_canvas(
            fill_color="rgba(255, 0, 0, 0.2)",
            stroke_width=2,
            stroke_color="#FF0000",
            background_image=image,
            update_streamlit=True,
            height=display_h,
            width=display_w,
            drawing_mode="rect",
            key="manual_canvas",
        )

        # Hi·ªÉn th·ªã s·ªë l∆∞·ª£ng v√πng ƒë√£ ch·ªçn
        if canvas_result.json_data is not None:
            num_boxes = len([obj for obj in canvas_result.json_data.get("objects", []) if obj["type"] == "rect"])
            if num_boxes > 0:
                st.success(f"üìç ƒê√£ ch·ªçn {num_boxes} v√πng ƒë·ªÉ x·ª≠ l√Ω")
            else:
                st.info("‚úèÔ∏è Ch∆∞a c√≥ v√πng n√†o ƒë∆∞·ª£c ch·ªçn. V·∫Ω h√¨nh ch·ªØ nh·∫≠t tr√™n ·∫£nh ƒë·ªÉ ch·ªçn v√πng c·∫ßn x·ª≠ l√Ω.")

        # ===== X·ª¨ L√ù V√Ä K·∫æT QU·∫¢ =====
        st.markdown("---")
        st.header("üöÄ X·ª≠ l√Ω")

        if canvas_result and canvas_result.json_data is not None:
            boxes = extract_boxes(canvas_result, display_w, display_h, orig_w, orig_h)
            
            if not boxes:
                st.warning("‚ö†Ô∏è Vui l√≤ng v·∫Ω √≠t nh·∫•t m·ªôt v√πng tr√™n ·∫£nh ƒë·ªÉ b·∫Øt ƒë·∫ßu x·ª≠ l√Ω")
            else:
                # N√∫t x·ª≠ l√Ω
                if st.button("üöÄ TƒÉng C∆∞·ªùng", type="primary", use_container_width=True, key="manual_enhance"):
                    st.markdown("---")
                    st.subheader(f"üîÑ X·ª≠ l√Ω {len(boxes)} v√πng...")
                    
                    # Progress bar
                    progress_bar = st.progress(0)
                    status_text = st.empty()
                    
                    # Container cho t·∫•t c·∫£ k·∫øt qu·∫£
                    results_container = st.container()
                    
                    with results_container:
                        # X·ª≠ l√Ω t·ª´ng v√πng
                        for i, box in enumerate(boxes):
                            # Update progress
                            progress = (i + 1) / len(boxes)
                            progress_bar.progress(progress)
                            status_text.text(f"ƒêang x·ª≠ l√Ω v√πng {i+1}/{len(boxes)}...")
                            
                            st.markdown(f"### üîç V√πng {i+1}")
                            
                            # Crop v√πng 32x32 t·ª´ center c·ªßa bounding box
                            region_32 = crop_center(image_np, box, crop_size=32)
                            
                            # Upscale l√™n 256x256 cho hi·ªÉn th·ªã
                            region_256_display = cv2.resize(region_32, (256, 256), interpolation=cv2.INTER_CUBIC)
                            
                            # Enhancement
                            enhanced = enhance_image_region(model, region_32)
                            
                            if enhanced is not None:
                                # Hi·ªÉn th·ªã k·∫øt qu·∫£ so s√°nh trong 2 c·ªôt
                                col_orig, col_enh = st.columns(2)
                                
                                with col_orig:
                                    st.markdown("**üî∏ ·∫¢nh G·ªëc**")
                                    st.image(region_256_display, use_column_width=True)
                                
                                with col_enh:
                                    st.markdown("**‚ú® ·∫¢nh ƒê√£ TƒÉng C∆∞·ªùng**")
                                    st.image(enhanced, use_column_width=True)
                                
                                # Th√™m separator gi·ªØa c√°c v√πng
                                if i < len(boxes) - 1:
                                    st.markdown("---")
                            else:
                                st.error(f"‚ùå Kh√¥ng th·ªÉ x·ª≠ l√Ω v√πng {i+1}")
                        
                        # Ho√†n th√†nh
                        progress_bar.progress(1.0)
                        status_text.text("‚úÖ Ho√†n th√†nh x·ª≠ l√Ω t·∫•t c·∫£ v√πng!")
                        st.success(f"üéâ ƒê√£ tƒÉng c∆∞·ªùng th√†nh c√¥ng {len(boxes)} v√πng!")
        else:
            st.info("‚úèÔ∏è V·∫Ω v√πng c·∫ßn tƒÉng c∆∞·ªùng tr√™n ·∫£nh, sau ƒë√≥ nh·∫•n n√∫t TƒÉng C∆∞·ªùng ƒë·ªÉ x·ª≠ l√Ω.")

# ========== TAB 2: DATASET AUTO ENHANCEMENT ==========
with tab2:
    st.header("üìä Dataset Auto Enhancement")
    st.markdown("""
    T√≠nh nƒÉng n√†y cho ph√©p t·ª± ƒë·ªông tƒÉng c∆∞·ªùng c√°c v√πng ƒë√£ ƒë∆∞·ª£c annotation s·∫µn trong dataset.
    H·ªó tr·ª£ nhi·ªÅu format annotation ph·ªï bi·∫øn.
    """)
    
    # ===== B∆Ø·ªöC 1: UPLOAD FILES =====
    st.markdown("---")
    st.subheader("üìÅ B∆∞·ªõc 1: Upload Files")
    
    col_img, col_ann = st.columns(2)
    
    with col_img:
        st.markdown("**Upload ·∫£nh X-ray:**")
        dataset_image = st.file_uploader("Ch·ªçn ·∫£nh t·ª´ dataset", 
                                       type=["png", "jpg", "jpeg", "bmp", "tiff"], 
                                       key="dataset_image")
    
    with col_ann:
        st.markdown("**Upload file annotation:**")
        annotation_file = st.file_uploader("Ch·ªçn file annotation", 
                                         type=["csv", "json", "xml", "txt"], 
                                         key="annotation_file")
    
    if not dataset_image or not annotation_file:
        st.info("üìÅ Vui l√≤ng upload c·∫£ ·∫£nh v√† file annotation ƒë·ªÉ ti·∫øp t·ª•c")
    else:
        # Load image
        image = Image.open(dataset_image).convert("RGB")
        image_np = np.array(image)
        orig_w, orig_h = image.size
        display_w, display_h = calculate_display_size(orig_w, orig_h)
        
        st.success(f"‚úÖ ƒê√£ upload ·∫£nh: {dataset_image.name} ({orig_w}x{orig_h})")
        
        # ===== B∆Ø·ªöC 2: PARSE ANNOTATION =====
        st.markdown("---")
        st.subheader("üîç B∆∞·ªõc 2: Parse Annotation")
        
        # Parse annotation based on file type
        file_ext = annotation_file.name.split('.')[-1].lower()
        boxes = None
        parse_message = ""
        
        if file_ext == 'csv':
            result, parse_message = parse_csv_annotation(annotation_file)
            if result is not None:
                boxes, classes = result
            else:
                boxes, classes = None, None
        elif file_ext == 'json':
            boxes, parse_message = parse_json_annotation(annotation_file, dataset_image.name)
        elif file_ext == 'xml':
            boxes, parse_message = parse_xml_annotation(annotation_file, dataset_image.name)
        elif file_ext == 'txt':
            boxes, parse_message = parse_txt_annotation(annotation_file, dataset_image.name, orig_w, orig_h)
        else:
            parse_message = f"Format {file_ext} ch∆∞a ƒë∆∞·ª£c h·ªó tr·ª£"
        
        if boxes is None:
            st.error(f"‚ùå {parse_message}")
            st.info("**C√°c format ƒë∆∞·ª£c h·ªó tr·ª£:**")
            st.markdown("""
            - **CSV**: filename, x, y, width, height (ho·∫∑c c√°c bi·∫øn th·ªÉ)
            - **JSON**: COCO format ho·∫∑c custom format
            - **XML**: PASCAL VOC format
            - **TXT**: YOLO format (normalized coordinates)
            """)
        else:
            st.success(f"‚úÖ {parse_message}")
            
            # Visualize annotations
            # st.markdown("**Xem tr∆∞·ªõc th√¥ng tin:**")
            img_with_boxes = visualize_annotations(image_np, boxes, classes)             
            # Display v·ªõi size ph√π h·ª£p
            # img_display = cv2.resize(img_with_boxes, (display_w, display_h))
            # st.image(img_display, caption=f"·∫¢nh g·ªëc{len(boxes)} annotation(s)", use_column_width=True) 
            # st.subheader(f"C√≥ {len(boxes)} v√πng")
            
            # Show annotation details
            with st.expander("üìã Chi ti·∫øt annotations", expanded=False):
                for i, (x1, y1, x2, y2) in enumerate(boxes):
                    if classes is not None and len(classes) > i:
                        st.write(
                            f"**V√πng {i+1}:** x1={x1}, y1={y1}, x2={x2}, y2={y2} "
                            f"(size: {x2-x1}x{y2-y1}) | class: {classes[i]['class_name']} (ID: {classes[i]['class_id']})"
                        )
                    else:
                        st.write(f"**V√πng {i+1}:** x1={x1}, y1={y1}, x2={x2}, y2={y2} (size: {x2-x1}x{y2-y1})")

            
            # ===== B∆Ø·ªöC 3: T·ª∞ ƒê·ªòNG TƒÇNG C∆Ø·ªúNG =====
            st.markdown("---")
            st.subheader("üöÄ B∆∞·ªõc 3: T·ª± ƒë·ªông tƒÉng c∆∞·ªùng")

            if st.button("üöÄ TƒÉng C∆∞·ªùng T·∫•t C·∫£ C√°c V√πng ƒê√£ ƒê∆∞·ª£c Annotation!", 
                        type="primary", use_container_width=True, key="auto_enhance"):
                
                st.markdown("---")
                st.subheader(f"üîÑ ƒêang x·ª≠ l√Ω {len(boxes)} v√πng ƒë∆∞·ª£c annotation...")
                
                # Progress bar
                progress_bar = st.progress(0)
                status_text = st.empty()
                
                # Container cho t·∫•t c·∫£ k·∫øt qu·∫£
                results_container = st.container()
                
                with results_container:
                    # X·ª≠ l√Ω t·ª´ng v√πng
                    for i, box in enumerate(boxes):
                        # Update progress
                        progress = (i + 1) / len(boxes)
                        progress_bar.progress(progress)
                        status_text.text(f"ƒêang x·ª≠ l√Ω v√πng annotation {i+1}/{len(boxes)}...")
                        
                        st.markdown(f"### üîç Annotation V√πng {i+1}")
                        
                        # Crop v√πng 32x32 t·ª´ center c·ªßa bounding box
                        region_32 = crop_center(image_np, box, crop_size=32)
                        
                        # Upscale l√™n 256x256 cho hi·ªÉn th·ªã
                        region_256_display = cv2.resize(region_32, (256, 256), interpolation=cv2.INTER_CUBIC)
                        
                        # Enhancement
                        enhanced = enhance_image_region(model, region_32)
                        
                        if enhanced is not None:
                            # Hi·ªÉn th·ªã k·∫øt qu·∫£ so s√°nh
                            col_orig, col_enh = st.columns(2)
                            
                            with col_orig:
                                st.markdown("**üî∏ ·∫¢nh G·ªëc**")
                                st.image(region_256_display, use_column_width=True)
                                x1, y1, x2, y2 = box
                                st.caption(f"V·ªã tr√≠: x1={x1}, y1={y1}, x2={x2}, y2={y2} (size: {x2-x1}x{y2-y1})")
                            
                            with col_enh:
                                st.markdown("**‚ú® ·∫¢nh ƒê√£ TƒÉng C∆∞·ªùng**")
                                st.image(enhanced, use_column_width=True)
                            
                            # Th√™m separator gi·ªØa c√°c v√πng
                            if i < len(boxes) - 1:
                                st.markdown("---")
                        else:
                            st.error(f"‚ùå Kh√¥ng th·ªÉ x·ª≠ l√Ω annotation v√πng {i+1}")
                    
                    # Ho√†n th√†nh
                    progress_bar.progress(1.0)
                    status_text.text("‚úÖ ƒê√£ ho√†n th√†nh tƒÉng c∆∞·ªùng t·∫•t c·∫£ v√πng annotation!")
                    st.success(f"üéâ ƒê√£ tƒÉng c∆∞·ªùng th√†nh c√¥ng {len(boxes)} v√πng annotation!")

                    # Ho√†n th√†nh
                    progress_bar.progress(1.0)
                    status_text.text("‚úÖ ƒê√£ ho√†n th√†nh tƒÉng c∆∞·ªùng t·∫•t c·∫£ v√πng annotation!")
                    st.success(f"üéâ ƒê√£ tƒÉng c∆∞·ªùng th√†nh c√¥ng {len(boxes)} v√πng annotation!")